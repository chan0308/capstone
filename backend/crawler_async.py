# crawler_async.py
import sys, os, json, asyncio
from typing import List, Dict
from playwright.async_api import async_playwright

# â–¶ Windowsì—ì„œ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ ì§€ì› ë£¨í”„ ì •ì±… ì ìš©(ì£¼í”¼í„° ë°– í”„ë¡œì„¸ìŠ¤ë¼ 100% ë°˜ì˜ë¨)
if sys.platform.startswith("win"):
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

STORAGE_PATH = "reddit_storage.json"
SEARCH_URL = "https://www.reddit.com/r/selfdrivingcars/search/?q=waymo&type=posts&t=year"
MAX_POSTS = 200  # âœ… ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜ ì œí•œ

async def solve_captcha_if_needed(page) -> None:
    """Cloudflare/Turnstile/hCaptcha ë“± 'ì‚¬ëŒ í™•ì¸' í™”ë©´ ëŒ€ì‘."""
    challenge_sel = (
        "iframe[src*='challenges.cloudflare.com'], "
        "iframe[src*='hcaptcha.com'], "
        "div[id*='challenge'], "
        "div[id*='captcha']"
    )
    feed_sel_any = [
        "a[data-testid='post-title']",
        "shreddit-post a[slot='title']",
        "shreddit-app",
        "#SHORTCUT_FOCUSABLE_DIV",
    ]

    for s in feed_sel_any:
        if await page.locator(s).first.is_visible():
            return

    has_challenge = await page.locator(challenge_sel).count() > 0
    if has_challenge:
        print("âš ï¸ ì‚¬ëŒì´ í™•ì¸í•´ì•¼ í•˜ëŠ” í™”ë©´ ê°ì§€ë¨ â€” ë¸Œë¼ìš°ì €ì—ì„œ í†µê³¼í•œ ë’¤ ì½˜ì†”ì— Enterë¥¼ ëˆŒëŸ¬ ê³„ì†í•˜ì„¸ìš”.")
        try:
            for _ in range(180):  # ìµœëŒ€ ~3ë¶„ ëŒ€ê¸°
                ok = any([await page.locator(s).first.is_visible() for s in feed_sel_any])
                if ok:
                    print("âœ… ë©”ì¸ í”¼ë“œ í™•ì¸. ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                    return
                if await page.locator(challenge_sel).count() == 0:
                    print("âœ… ì±Œë¦°ì§€ í”„ë ˆì„ì´ ì‚¬ë¼ì¡ŒìŠµë‹ˆë‹¤. ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                    return
                await asyncio.sleep(1.5)
        except KeyboardInterrupt:
            pass
        try:
            input("ğŸ‘‰ í†µê³¼ë¥¼ ë§ˆì³¤ë‹¤ë©´ ì—¬ê¸°ì„œ Enterë¥¼ ëˆŒëŸ¬ ê³„ì†í•˜ì„¸ìš”...")
        except Exception:
            pass

async def ensure_context(browser):
    if os.path.exists(STORAGE_PATH):
        print(f"ğŸ” ì €ì¥ëœ ì„¸ì…˜ ë¡œë“œ: {STORAGE_PATH}")
        return await browser.new_context(storage_state=STORAGE_PATH)
    else:
        print("ğŸ†• ìƒˆ ì„¸ì…˜ ì‹œì‘ (reddit_storage.json ì—†ìŒ)")
        return await browser.new_context()

async def detect_article_selector(page) -> str:
    try:
        await page.wait_for_selector('a[data-testid="post-title"]', timeout=7000)
        print("âœ… êµ¬ Reddit ì…€ë ‰í„° ê°ì§€ë¨")
        return 'a[data-testid="post-title"]'
    except:
        await page.wait_for_selector("shreddit-post a[slot='title']", timeout=10000)
        print("âœ… ì‹  Reddit ì…€ë ‰í„° ê°ì§€ë¨")
        return "shreddit-post a[slot='title']"

async def click_load_more_if_any(page) -> bool:
    """ê²€ìƒ‰ ê²°ê³¼ í•˜ë‹¨ì˜ 'ë”ë³´ê¸°' ë²„íŠ¼ì´ ìˆë‹¤ë©´ í´ë¦­."""
    selectors = [
        "button:has-text('Load more results')",
        "button:has-text('Load More Results')",
        "button:has-text('Load more')",
        "shreddit-load-more button",
    ]
    for s in selectors:
        loc = page.locator(s).first
        if await loc.count() > 0 and await loc.is_visible():
            try:
                await loc.click()
                await asyncio.sleep(2.0)
                return True
            except:
                pass
    return False

async def infinite_scroll(page, article_selector: str, max_rounds: int = 120, sleep_sec: float = 1.6, max_posts: int = MAX_POSTS):
    """
    - ë‚´ë¶€ ìŠ¤í¬ë¡¤ ì»¨í…Œì´ë„ˆê¹Œì§€ ê³ ë ¤í•˜ì—¬ ìŠ¤í¬ë¡¤
    - ë§¤ ë¼ìš´ë“œ 'ë”ë³´ê¸°' ë²„íŠ¼ ì‹œë„
    - ê²Œì‹œë¬¼ ìˆ˜ê°€ max_postsì— ë„ë‹¬í•˜ë©´ ì¦‰ì‹œ ì¢…ë£Œ
    - ì¦ê°€ ì—†ìŒì„ ëª‡ íšŒ ê°ì§€í•˜ë©´ ì¢…ë£Œ
    """
    scroll_candidates = [
        "#SHORTCUT_FOCUSABLE_DIV",        # êµ¬ Reddit
        "#AppRouter-main-content",        # ì‹  Reddit ë‚´ë¶€ ì»¨í…Œì´ë„ˆ
        "shreddit-app",                   # ì‹  Reddit ë£¨íŠ¸
        "html", "body",                   # í´ë°±
    ]

    async def do_scroll_once() -> int:
        scrolled = 0
        for sel in scroll_candidates:
            try:
                count = await page.locator(sel).count()
                if count == 0:
                    continue
                await page.evaluate(
                    """(sel) => {
                        const el = document.querySelector(sel);
                        if (!el) return;
                        const target = (el === document.documentElement || el === document.body)
                                      ? (document.scrollingElement || document.documentElement)
                                      : el;
                        target.scrollTop = target.scrollHeight;
                    }""",
                    sel
                )
                scrolled += 1
            except:
                pass
        return scrolled

    prev_count = 0
    stable_rounds = 0

    for r in range(1, max_rounds + 1):
        try:
            count_now = await page.locator(article_selector).count()
        except:
            count_now = prev_count

        print(f"ğŸ”„ ìŠ¤í¬ë¡¤ ë¼ìš´ë“œ {r} | ê²Œì‹œë¬¼ {count_now}ê°œ ê°ì§€ (ëª©í‘œ {max_posts})")

        # ëª©í‘œ ë‹¬ì„± ì‹œ ì¢…ë£Œ
        if count_now >= max_posts:
            print("ğŸ¯ ëª©í‘œ ê°œìˆ˜ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ìŠ¤í¬ë¡¤ ì¢…ë£Œ.")
            break

        # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­
        clicked = await click_load_more_if_any(page)
        if clicked:
            await asyncio.sleep(2.5)

        # ìŠ¤í¬ë¡¤
        await do_scroll_once()
        await asyncio.sleep(sleep_sec)

        # ì¦ê°€ ì²´í¬
        try:
            new_count = await page.locator(article_selector).count()
        except:
            new_count = count_now

        if new_count <= count_now:
            stable_rounds += 1
        else:
            stable_rounds = 0

        prev_count = new_count

        if stable_rounds >= 3:
            print("â›”ï¸ ë” ì´ìƒ ìƒˆë¡œìš´ ê²Œì‹œë¬¼ì´ ëŠ˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìŠ¤í¬ë¡¤ ì¢…ë£Œ.")
            break

async def extract_posts(page, article_selector: str, max_posts: int = MAX_POSTS) -> List[Dict]:
    """í˜„ì¬ ë¡œë“œëœ ê²Œì‹œë¬¼ì—ì„œ ìµœëŒ€ max_postsê¹Œì§€ ì¶”ì¶œ."""
    elements = await page.locator(article_selector).all()
    print(f"\nğŸ“° ê°ì§€ëœ Tesla ê²Œì‹œë¬¼(í˜„ì¬ DOM): {len(elements)}ê°œ\n{'='*80}")
    posts: List[Dict] = []
    idx = 0
    for el in elements:
        if idx >= max_posts:
            break
        try:
            title = await el.get_attribute("aria-label")
            if not title:
                txt = await el.text_content()
                title = (txt or "").strip()
            href = await el.get_attribute("href")
            if not href:
                continue
            if href.startswith("/"):
                href = f"https://www.reddit.com{href}"
            idx += 1
            posts.append({"index": idx, "title": title, "url": href})
            print(f"{idx:3d}. {title}\n    ğŸ”— {href}")
        except Exception as e:
            idx += 1
            print(f"{idx:3d}. âŒ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    print("=" * 80)
    print(f"âœ… ì´ {len(posts)}ê°œì˜ ê²Œì‹œë¬¼ ìˆ˜ì§‘ ì™„ë£Œ! (ìƒí•œ {max_posts})")
    return posts

async def main():
    print("ğŸš€ Tesla ê²Œì‹œë¬¼ ì¶”ì¶œ ì‹œì‘!")
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False, args=["--start-maximized"])
        context = await ensure_context(browser)
        page = await context.new_page()

        print("ğŸŒ Reddit í™ˆ ì ‘ì† ì¤‘... (ìº¡ì°¨ ë°œìƒ ì‹œ ì§ì ‘ í†µê³¼ í•„ìš”)")
        await page.goto("https://www.reddit.com/", timeout=60000, wait_until="load")
        await asyncio.sleep(1.0)
        await solve_captcha_if_needed(page)

        try:
            await context.storage_state(path=STORAGE_PATH)
            print(f"ğŸ’¾ ì„¸ì…˜ ì €ì¥ ì™„ë£Œ: {STORAGE_PATH}")
        except Exception as e:
            print("âš ï¸ ì„¸ì…˜ ì €ì¥ ê²½ê³ :", e)

        print("ğŸ” ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™:", SEARCH_URL)
        await page.goto(SEARCH_URL, timeout=60000, wait_until="networkidle")
        await asyncio.sleep(2.0)
        await solve_captcha_if_needed(page)

        article_selector = await detect_article_selector(page)
        await infinite_scroll(page, article_selector, max_rounds=120, sleep_sec=1.6, max_posts=MAX_POSTS)
        posts = await extract_posts(page, article_selector, max_posts=MAX_POSTS)

        out_path = "Tesla_posts.json"
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(posts, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ: {out_path}")

        await context.close()
        await browser.close()
        return posts

if __name__ == "__main__":
    res = asyncio.run(main())
    print(f"\nğŸ“¦ ìˆ˜ì§‘ëœ posts ê°œìˆ˜: {len(res)}")
    for p in res[:3]:
        print(f" - {p['title']} ({p['url']})")
